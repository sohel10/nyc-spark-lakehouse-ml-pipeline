# ğŸš• NYC Spark Lakehouse & ML Pipeline

Scalable PySpark-based data engineering and machine learning pipeline built on multi-year NYC Yellow Taxi trip data.

This project demonstrates distributed data processing, schema harmonization, partition optimization, and ML-ready dataset construction using Spark.

---

## ğŸ“Œ Project Overview

NYC Yellow Taxi data is published in monthly parquet files across multiple years.  
Although already in parquet format, the schema evolves over time and requires harmonization for large-scale analytical processing.

This project implements a **production-style lakehouse architecture**:

- Handles schema drift across years
- Resolves datatype inconsistencies
- Avoids Spark memory crashes during ingestion
- Controls partitioning to optimize file sizes
- Builds a partitioned analytical dataset for ML workloads

---

## ğŸ—ï¸ Architecture

data_raw/ â†’ Original monthly parquet files
data_clean_tmp/ â†’ Memory-safe ingestion layer (streaming month-level processing)
data_clean/ â†’ Standardized yearly datasets
data_processed/ â†’ Partitioned analytical dataset (year/month)
models/ â†’ ML training & artifacts


---

## ğŸ”§ Engineering Highlights

### âœ… Schema Harmonization
- Used `unionByName(allowMissingColumns=True)` to handle evolving schemas
- Resolved parquet physical/logical type mismatches
- Standardized numeric casting across years

### âœ… Memory-Safe Processing
- Streamed ingestion at month-level to prevent `Java heap space` crashes
- Avoided full-year in-memory unions
- Used controlled `repartition` strategy for scalable writes

### âœ… Partition Optimization
- Balanced file size and distributed performance
- Prevented small-file problem
- Designed partitioned analytical layer for partition pruning

### âœ… Lakehouse Design
- Raw â†’ Clean â†’ Processed layering
- ML-ready dataset construction
- Separation of ingestion and modeling concerns

---

## âš¡ Technology Stack

- **PySpark**
- **Parquet**
- **Distributed Data Processing**
- **Lakehouse Architecture**
- **Partition Optimization**
- **Feature Engineering**

---

## ğŸ“Š ML-Ready Dataset

The final `data_processed/` layer is partitioned by:

year=YYYY/
month=MM/


This enables:

- Efficient distributed training
- Partition pruning
- Faster analytical queries
- Scalable model experimentation

---

## ğŸš€ How to Run


This enables:

- Efficient distributed training
- Partition pruning
- Faster analytical queries
- Scalable model experimentation

---

## ğŸš€ How to Run


This enables:

- Efficient distributed training
- Partition pruning
- Faster analytical queries
- Scalable model experimentation

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Clean Monthly Data (Memory-Safe)
```bash
python -m jobs.clean_year_tmp

ğŸ“œ License

MIT License
---

# ğŸ”¥ This README Signals

âœ” Data engineering maturity  
âœ” Distributed systems understanding  
âœ” ML pipeline readiness  
âœ” Production architecture thinking  

---

If you'd like, next I can:

- Add an architecture diagram (visual style)
- Add performance benchmarking section
- Add ML model training section
- Optimize for recruiter keywords
- Create a professional GitHub profile summary

Tell me which direction you want next ğŸš€