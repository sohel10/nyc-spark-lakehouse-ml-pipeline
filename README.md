# ğŸš• NYC Spark Lakehouse & ML Pipeline

Production-style distributed data engineering pipeline built with PySpark, simulating real-world multi-year big data ingestion, schema evolution management, and ML-ready dataset preparation using NYC Yellow Taxi data.

This project demonstrates distributed data processing, schema harmonization, partition optimization, and ML-ready dataset construction using Spark.

---

## ğŸ§° Tech Stack

[![Python](https://img.shields.io/badge/Python-3.10+-blue)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-Distributed%20Processing-orange)](https://spark.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.x-E25A1C)](https://spark.apache.org/)
[![Parquet](https://img.shields.io/badge/Storage-Parquet-4B8BBE)](https://parquet.apache.org/)
[![Lakehouse](https://img.shields.io/badge/Architecture-Lakehouse-blue)]()
[![Distributed Systems](https://img.shields.io/badge/Concept-Distributed%20Systems-lightgrey)]()
[![Ubuntu](https://img.shields.io/badge/OS-Ubuntu-FCC624)](https://ubuntu.com/)
[![Git](https://img.shields.io/badge/Version%20Control-Git-F05032)](https://git-scm.com/)

## ğŸ“Œ Project Overview

NYC Yellow Taxi data is published in monthly parquet files across multiple years.  
Although already in parquet format, the schema evolves over time and requires harmonization for large-scale analytical processing.

This project implements a **production-style lakehouse architecture**:

- Handles schema drift across years
- Resolves datatype inconsistencies
- Avoids Spark memory crashes during ingestion
- Controls partitioning to optimize file sizes
- Builds a partitioned analytical dataset for ML workloads

---

## ğŸ—ï¸ Architecture
## ğŸ— Architecture Overview

This project implements a production-style lakehouse architecture for scalable multi-year data ingestion and analytical dataset preparation.

## âš™ Example Execution

Below shows a diagran ingestion and merge process executed with PySpark:

![Spark Pipeline](docs/pipeline.png)


Each layer isolates responsibilities:
- Raw ingestion
- Schema harmonization
- Partition optimization
- Analytical dataset construction

## ğŸ“Š Dataset Scale

The pipeline processes large-scale NYC Yellow Taxi trip data:

- **17,089,605 records** ingested and harmonized
- Year-level standardized parquet dataset
- Optimized across **72 Spark partitions**
- Partition-aware dataset design for distributed execution
- Structured for scalable analytical and ML-ready workflows

## âš™ Example Execution

Below shows a successful yearly ingestion and merge process executed with PySpark:

![Spark Execution](docs/spark.png)

## âš¡ Performance Considerations

To ensure scalability across multi-year data:

- Avoided full-year in-memory unions
- Streamed ingestion at month-level
- Used controlled `repartition()` strategy
- Designed partitioned write layer for pruning
- Prevented small-file explosion problem
- Minimized Spark shuffle operations
---

## ğŸ”„ Schema Evolution Handling

NYC taxi parquet schemas evolve across years.

Key solutions implemented:

- Used `unionByName(allowMissingColumns=True)`
- Standardized numeric casting across years
- Resolved physical/logical parquet type mismatches
- Explicit column alignment before union

## ğŸ“‚ Project Structure
```` text
nyc-spark-lakehouse-ml-pipeline/
â”‚
â”œâ”€â”€ jobs/              # Spark job orchestration scripts
â”œâ”€â”€ src/               # Core transformations & schema logic
â”œâ”€â”€ data_raw/          # Raw monthly parquet files (ignored)
â”œâ”€â”€ data_clean_tmp/    # Memory-safe ingestion layer
â”œâ”€â”€ data_clean/        # Harmonized yearly datasets
â”œâ”€â”€ data_processed/    # Partitioned analytical dataset
â”œâ”€â”€ docs/              # Architecture & execution images
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md

````

## ğŸ¯ What This Project Demonstrates

- Distributed data engineering workflows
- Handling large-scale multi-year datasets
- Schema drift resolution
- Spark memory management strategies
- Partition-aware dataset design
- Lakehouse architectural layering
- Production-style pipeline structuring


## ğŸ”§ Engineering Highlights

### âœ… Schema Harmonization
- Used `unionByName(allowMissingColumns=True)` to handle evolving schemas
- Resolved parquet physical/logical type mismatches
- Standardized numeric casting across years

### âœ… Memory-Safe Processing
- Streamed ingestion at month-level to prevent `Java heap space` crashes
- Avoided full-year in-memory unions
- Used controlled `repartition` strategy for scalable writes

### âœ… Partition Optimization
- Balanced file size and distributed performance
- Prevented small-file problem
- Designed partitioned analytical layer for partition pruning

### âœ… Lakehouse Design
- Raw â†’ Clean â†’ Processed layering
- ML-ready dataset construction
- Separation of ingestion and modeling concerns

---

## âš¡ Technology Stack

- **PySpark**
- **Parquet**
- **Distributed Data Processing**
- **Lakehouse Architecture**
- **Partition Optimization**
- **Feature Engineering**

---

## ğŸ“Š ML-Ready Dataset

The final `data_processed/` layer is partitioned by:

year=YYYY/
month=MM/


This enables:

- Efficient distributed training
- Partition pruning
- Faster analytical queries
- Scalable model experimentation

---

## ğŸš€ How to Run


This enables:

- Efficient distributed training
- Partition pruning
- Faster analytical queries
- Scalable model experimentation

---

## ğŸš€ How to Run


This enables:

- Efficient distributed training
- Partition pruning
- Faster analytical queries
- Scalable model experimentation

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Clean Monthly Data (Memory-Safe)
```bash
python -m jobs.clean_year_tmp

ğŸ“œ License

MIT License
---

# ğŸ”¥ This README Signals

âœ” Data engineering maturity  
âœ” Distributed systems understanding  
âœ” ML pipeline readiness  
âœ” Production architecture thinking  

---

If you'd like, next I can:

- Add an architecture diagram (visual style)
- Add performance benchmarking section
- Add ML model training section
- Optimize for recruiter keywords
- Create a professional GitHub profile summary

Tell me which direction you want next ğŸš€